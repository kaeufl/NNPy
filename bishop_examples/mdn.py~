# -*- coding: utf-8 -*-
class MDN(TLP):
  def __init__(self, H = 3, d = 1, ny = 1, M = 3, debug_output = False):
    """
    H: number of hidden units
    d: number of inputs
    ny: number of outputs
    M: number of mixture components
    """
    self.c = ny
    ny = 2*M + M * ny # M mixing coefficients + M variances + M*ny means
    self.M = M
    self.count_fwd = 0
    TLP.__init__(self, H, d, ny, linear_output = True, error_function = 'mdn', 
                   debug_output = debug_output)
    
  def init_weights(self, t, prior):
    """
    initialze weights and biases so that the network models the unconditional density 
    of the target data p(t)
    t: target data
    prior: 1/sigma^2
    """
    from scipy.cluster.vq import kmeans2
    from scipy.spatial.distance import cdist
    sigma = np.sqrt(1.0/prior)
    # init weights from gaussian with width given by prior
    self.w1 = np.random.normal(loc=0.0, scale = 1,size=[self.H, self.d+1])*sigma # 1st layer weights + bias
    self.w2 = np.random.normal(loc=0.0, scale = 1,size=[self.ny, self.H+1])*sigma # 2nd layer weights + bias
    # init biases (taken from netlab, gmminit.m)
    [centroid, label] = kmeans2(t, self.M)
    cluster_sizes = np.maximum(np.bincount(label), 1) # avoid empty clusters
    alpha = cluster_sizes/np.sum(cluster_sizes)
    if (self.M > 1):
      # estimate variance from the distance to the nearest centre
      sigma = cdist(centroid, centroid)
      sigma = np.min(sigma + np.diag(np.diag(np.ones(sigma.shape))) * 1000, 1)
      sigma = np.maximum(sigma, np.finfo(float).eps) # avoid underflow
    else:
      # only one centre: take average variance
      sigma = np.mean(np.diag([np.var(t)]))
    # set biases, taken from netlab, mdninit.m
    self.w2[0:self.M,0] = alpha
    self.w2[self.M:2*self.M,0] = np.log(sigma)
    self.w2[2*self.M:,0] = np.reshape(centroid, [self.M * self.c])
  
  def getMixtureParams(self, y):
    """Returns the parameters of the gaussian mixture."""
    if len(y.shape) == 1:
      # avoid underrun
      alpha = np.maximum(y[0:self.M], np.finfo(float).eps)
      sigma = y[self.M:2*self.M]
      mu = np.reshape(y[2*self.M:], [self.c, self.M]).T
    else:
      # avoid underrun
      alpha = np.maximum(y.T[0:self.M], np.finfo(float).eps)
      sigma = y.T[self.M:2*self.M]
      mu = np.reshape(y[:, 2*self.M:], [y.shape[0], self.c, self.M]).T
    return alpha, sigma, mu
  
  def _phi(self, T, mu, sigma):
    M = int(self.M)
    # distance between target data and gaussian kernels    
    dist = np.sum((T-mu)**2, 1)
    phi = (1.0 / (2*np.pi*sigma)**(0.5*self.c)) * np.exp(- 1.0 * dist / (2 * sigma))
    # prevent underflow
    return np.maximum(phi, np.finfo(float).eps)
    
  def E_mdn(self, y, t, w1, w2):
    """mdn error function"""
    M = int(self.M)
    alpha, sigma, mu = self.getMixtureParams(y.T)
    T = np.tile(t.T, [M,1,1])
    phi = self._phi(T, mu, sigma)
    probs = np.maximum(np.sum(alpha * phi, 0), np.finfo(float).eps)
    return - np.log(probs)
  
  def dE_mdn(self, x, y, t, w1 = None, w2 = None):
    """derivative of mdn error function"""
    if w2 == None:
      w2 = self.w2
    M = int(self.M)
    # avoid underrun
    alpha, sigma, mu = self.getMixtureParams(y.T)
    #import pdb; pdb.set_trace()
    T = np.tile(t.T, [M,1,1])
    phi = self._phi(T, mu, sigma)
    pi = alpha * phi / np.sum(alpha * phi, 0)
    
    # derivatives of E with respect to the output variables (s. Bishop 1995, chp. 6.4)
    dE_dy_alpha = alpha - pi
    dE_dy_sigma = - 0.5 * pi * ((np.sum((T-mu)**2 , 1) / sigma) - self.c)
    dE_dy_mu = pi[:,np.newaxis,:] * (mu - T) / sigma[:,np.newaxis,:]

    dk = np.zeros([self.ny, x.shape[0]])
    dk[0:M,:] = dE_dy_alpha
    dk[M:2*M,:] = dE_dy_sigma
    dk[2*M:] = np.reshape(dE_dy_mu, [M*self.c, x.shape[0]])
    
    # back-propagate the dks
    dj = (1 - self.z[1:]**2) * np.dot(w2.T[1:,:], dk)
    # evaluate derivatives with respect to the weights
    dEnw1 = (dj[:,:,np.newaxis]*x[np.newaxis,:,:]).transpose(1,0,2)
    dEnw2 = (dk[:,:,np.newaxis]*self.z.T[np.newaxis,:,:]).transpose(1,0,2)
    return dEnw1, dEnw2
    
  def _tlp(self, x, w1 = None, w2 = None):
    # perform forward propagation    
    #t0=datetime.now()
    y = TLP._tlp(self, x, w1, w2)
    
    #self.count_fwd = self.count_fwd + 1
    
    # the outputs are ordered as follows:
    # y[0:M]:       M mixing coefficients alpha
    # y[M:2*M]:     M kernel widths sigma
    # y[2*M:M*c]:   M x c kernel centre components
    M = int(self.M)
    # calculate mixing coefficients, variances and means from network output
    y[0:M] = self.softmax(y[0:M]) # alpha
    # avoid overflow
    y[M:2*M] = np.minimum(y[M:2*M], np.log(np.finfo(float).max))
    y[M:2*M] = np.exp(y[M:2*M]) # sigma
    # avoid underflow
    y[M:2*M] = np.maximum(y[M:2*M], np.finfo(float).eps)
    #print (datetime.now()-t0).microseconds
    return y
    

def plotPost2D(mdn, y, 
                rangex = [0, 1], rangey = [0, 1], 
                deltax = 0.01, deltay = 0.01,
                true_model = None):
  M = mdn.M
  alpha, sigma, mu = mdn.getMixtureParams(y)
  print 'mu: ' + str(mu)
  print 'sigma: ' + str(sigma)
  print 'true value: ' + str(true_model)
  xlin = np.arange(rangex[0], rangex[1], deltax)
  ylin = np.arange(rangey[0], rangey[1], deltay)
  [XLIN, YLIN] = np.meshgrid(xlin, ylin)
  
  phi = np.zeros([M,ylin.shape[0], xlin.shape[0]])
  P = np.zeros([ylin.shape[0], xlin.shape[0]])
  for k in range(M):
    phi[k,:,:] = mlab.bivariate_normal(XLIN, YLIN, np.sqrt(sigma[k]), np.sqrt(sigma[k]), mu[k,0], mu[k,1])
    P = P + phi[k,:,:] * alpha[k]
  plt.imshow(P, #interpolation='bilinear', 
            ##cmap=cm.gray,
            origin='lower', 
            extent=[rangex[0],rangex[1],
                    rangey[0],rangey[1]]
            )
  #plt.contour(XLIN, YLIN, P, 
              #levels = [0, 1.0/np.exp(1)]
  #            )
  #plt.scatter(true_model[0],true_model[1],marker='^', c="r")
  if not true_model == None:
    plt.axvline(true_model[0], c = 'r')
    plt.axhline(true_model[1], c = 'r')
  
def plotPost1D(mdn, y, rangex = [0, 1], deltax = 0.01, true_model = None):
  alpha, sigma, mu = mdn.getMixtureParams(y)
  xlin = np.arange(rangex[0], rangex[1], deltax)
  phi = np.zeros([mdn.M,xlin.shape[0]])
  P = np.zeros([xlin.shape[0]])
  for k in range(mdn.M):
    phi[k, :] = (1.0 / (2*np.pi*sigma[k])**(0.5)) * np.exp(- 1.0 * (xlin-mu[k,0])**2 / (2 * sigma[k]))
    P = P + phi[k, :] * alpha[k]
  #import pdb; pdb.set_trace()
  plt.plot(xlin, P)
  if true_model != None:
    plt.axvline(true_model, c = 'r')

def plotPostCond(mdn, x, t):
  y = mdn.forward(x)
  alpha, sigma, mu = mdn.getMixtureParams(y)
  N = t.shape[0]
  phi = np.zeros([mdn.M, N, N])
  P = np.zeros([N, N])
  T = np.tile(t, [N,1])
  for k in range(mdn.M):
    SIGMA = np.tile(sigma[k,:], [N, 1]).T
    MU = np.tile(mu[k,0,:], [N, 1]).T
    phi[k,:,:] = (1.0 / (2 * np.pi * SIGMA)**(0.5)) * np.exp(- 1.0 * (T-MU)**2 / (2 * SIGMA))
    P = P + phi[k,:,:] * np.tile(alpha[k, :], [N, 1]).T
  plt.imshow(P, #interpolation='bilinear', 
           #cmap=cm.gray,
           origin='lower', 
           extent=[min(t),max(t),
                   min(t),max(t)]
           )
  #X, Y = np.meshgrid(t, t)
  #plt.contour(X, Y, P, 
              #levels = [0, 1.0/np.exp(1)]
             #)